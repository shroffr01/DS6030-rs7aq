---
title: "Homework #1: Supervised Learning"
author: "**Rohan Shroff**"
format: ds6030hw-html
---

```{r config}
#| include: false
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages}
#| message: false
#| warning: false
library(tidyverse) # functions for data manipulation
```

# Problem 1: Evaluating a Regression Model

## a. Data generating functions

Create a set of functions to generate data from the following distributions:

```{=tex}
\begin{align*}
X &\sim \mathcal{N}(0, 1) \\
Y &= -1 + .5X + .2X^2 + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma)
\end{align*}
```
::: {.callout-note title="Solution"}
```{r}
# generates random x values
sim_x <- function(n) {
  runif(n)
}

# defines function y
f <- function(x) {
  -1 + .5*x + .2*(x^2)
}

# generates y values based on x
sim_y <- function(x, sd) {
  n = length(x)
  f(x) + rnorm(n, sd = sd)
}
```
:::

## b. Generate training data

Simulate $n=100$ realizations from these distributions using $\sigma=3$. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$.

-   Use `set.seed(611)` prior to generating the data.

::: {.callout-note title="Solution"}
```{r}
n <-  100
sd <-  3

set.seed(611)

x <- sim_x(n)
y <- sim_y(x, sd = sd)

data_train <- tibble(x,y)
```

```{r}
ggplot(data_train, aes(x,y)) + geom_point() + 
  geom_function(fun = f) +
  scale_x_continuous(breaks=seq(0,1, by=.10)) +
  scale_y_continuous(breaks=seq(-50,50,by=2))


```
:::

## c. Fit three models

Fit three polynomial regression models using least squares: linear, quadratic, and cubic. Produce another scatterplot, add the fitted lines and true population line $f(x)$ using different colors, and add a legend that maps the line color to a model.

-   Note: The true model is quadratic, but we are also fitting linear (less complex) and cubic (more complex) models.

::: {.callout-note title="Solution"}
```{r}
# fitting all three models
deg1_model = lm(y~poly(x, degree=1), data=data_train)
deg2_model = lm(y~poly(x, degree=2), data=data_train)
deg3_model = lm(y~poly(x, degree=3), data=data_train)
```

```{r}
deg1_model$coefficients
```

```{r}
ggplot(data_train, aes(x,y)) + geom_point() + 
  geom_function(fun = f) +
  scale_x_continuous(breaks=seq(0,1, by=.1)) +
  scale_y_continuous(breaks=seq(-50,50,by=2))

```
:::

## d. Predictive performance

Generate a *test data* set of 10,000 observations from the same distributions. Use `set.seed(612)` prior to generating the test data.

-   Calculate the estimated mean squared error (MSE) for each model.
-   Are the results as expected?

::: {.callout-note title="Solution"}
```{r}
n_test <- 10000
set.seed(612)

x_test <- sim_x(n_test)
y_test <- sim_y(x_test, sd=sd)
data_test <- tibble(x=x_test, y=y_test)
```

```{r}
#: Function to fit, predict, and evaluate a polynomial regression model

poly_eval <- function(deg, data_train, data_test){

if(deg==0) m = lm(y~1, data=data_train) # intercept only model

else m = lm(y~poly(x, degree=deg), data=data_train) # polynomial
p = length(coef(m)) # number of parameters
#: calculate training MSE
mse_train = mean(m$residuals^2) # training MSE
#: calculate test MSE
yhat = predict(m, data_test) # predictions at test X's
mse_test = mean( (data_test$y - yhat)^2 )# test MSE
#: output a data frame of relevant info
tibble(degree=deg, edf=p, mse_train, mse_test)
}
```

```{r}
poly_eval(deg=1, data_train, data_test)
poly_eval(deg=2, data_train, data_test)
poly_eval(deg=3, data_train, data_test)

```

The results are not as expected. The lowest MSE on the test data occurs when the degree = 1, not 2. This suggests that we are dealing with a linear model not a quadratic one.

```{r}
data_test
```
:::

## e. Optimal performance

What is the best achievable MSE? That is, what is the MSE if the true $f(x)$ was used to evaluate the test set? How close does the best method come to achieving the optimum?

::: {.callout-note title="Solution"}
```{r}

```

```{r}
# need to generate yhat values based on function
#yhat = predict(m, data_test) # predictions at test X's
#mse_test = mean( (data_test$y - yhat)^2 )# test MSE

# function that generates yhat values using true function
gen_yhat <- function(data_test) {
  
yhat_list <- vector()

for (i in 1:nrow(data_test)){
  
  yhat <- -1 + .5*data_test$x[i] + .2*(data_test$x[i]^2)
  
  yhat_list[i] <- yhat
}

yhat_list
}

yhat_list <- gen_yhat(data_test)

# get the test MSE
mse_test <- mean((data_test$y - yhat_list)^2)
```

```{r}
mse_test
```

This method predictably results in a lower MSE value. The best method out of the 3 models fitted to the data, the linear model, has a test MSE of 9.17. Therefore, a difference in MSE of .30.
:::

## f. Replication

The MSE scores obtained in part *d* came from one realization of training data. Here will we explore how much variation there is in the MSE scores by replicating the simulation many times.

-   Re-run parts b. and c. (i.e., generate training data and fit models) 100 times.
    -   Do not generate new testing data
    -   Use `set.seed(613)` prior to running the simulation and do not set the seed in any other places.
-   Calculate the test MSE for all simulations.
    -   Use the same test data from part d. (This question is only about the variability that comes from the *training data*).
-   Create kernel density or histogram plots of the resulting MSE values for each model.

::: {.callout-note title="Solution"}
```{r}
set.seed(613)

MSE_deg1 <- vector()
MSE_deg2 <- vector()
MSE_deg3 <- vector()

for (i in 1:100){
  
  n <-  100
  sd <-  3

  x <- sim_x(n)
  y <- sim_y(x, sd = sd)

  data_train <- tibble(x,y)
  
  deg1_model = lm(y~poly(x, degree=1), data=data_train)
  deg2_model = lm(y~poly(x, degree=2), data=data_train)
  deg3_model = lm(y~poly(x, degree=3), data=data_train)
  
  # caculate test MSE
  
  yhat_1 = predict(deg1_model, data_test) # predictions at test X's
  mse_test_deg1 = mean( (data_test$y - yhat_1)^2 )# test MSE
  
  yhat_2 = predict(deg2_model, data_test) # predictions at test X's
  mse_test_deg2 = mean( (data_test$y - yhat_2)^2 )# test MSE

  yhat_3 = predict(deg3_model, data_test) # predictions at test X's
  mse_test_deg3 = mean( (data_test$y - yhat_3)^2 )# test MSE

  MSE_deg1[i] <- mse_test_deg1
  MSE_deg2[i] <- mse_test_deg2
  MSE_deg3[i] <- mse_test_deg3
}

```

```{r}
degree <- c(rep(1, length(MSE_deg1)),rep(2, length(MSE_deg2)),rep(3, length(MSE_deg3)))
                
MSE <- c(MSE_deg1, MSE_deg2, MSE_deg3)
kd_df <- data.frame(degree, MSE)
kd_df$degree <- as.factor(kd_df$degree)

```

```{r}
ggplot(kd_df, aes(x=MSE, fill=degree)) +
  geom_density(alpha=0.3) +
  scale_x_continuous(breaks=seq(7,11, by=.5), limits = c(8.5, 11)) +
  scale_y_continuous(breaks=seq(0,50,by=2))
```
:::

## g. Best model

Show a count of how many times each model was the best. That is, out of the 100 simulations, count how many times each model had the lowest MSE.

::: {.callout-note title="Solution"}
```{r}
set.seed(613)

count_best_model1 <- 0
count_best_model2 <- 0
count_best_model3 <- 0

MSE_deg1 <- vector()
MSE_deg2 <- vector()
MSE_deg3 <- vector()

for (i in 1:100){
  
  n <-  100
  sd <-  3

  x <- sim_x(n)
  y <- sim_y(x, sd = sd)

  data_train <- tibble(x,y)
  
  deg1_model = lm(y~poly(x, degree=1), data=data_train)
  deg2_model = lm(y~poly(x, degree=2), data=data_train)
  deg3_model = lm(y~poly(x, degree=3), data=data_train)
  
  # caculate test MSE
  
  yhat_1 = predict(deg1_model, data_test) # predictions at test X's
  mse_test_deg1 = mean( (data_test$y - yhat_1)^2 )# test MSE
  
  yhat_2 = predict(deg2_model, data_test) # predictions at test X's
  mse_test_deg2 = mean( (data_test$y - yhat_2)^2 )# test MSE

  yhat_3 = predict(deg3_model, data_test) # predictions at test X's
  mse_test_deg3 = mean( (data_test$y - yhat_3)^2 )# test MSE
  
  if (count_best_model1 < count_best_model2 & count_best_model1 < count_best_model2){
    count_best_model1 <- count_best_model1 + 1
  }
  
  else if (count_best_model2 < count_best_model1 & count_best_model2 < count_best_model3){
    count_best_model2 <- count_best_model2 + 1
  }
  else if (count_best_model3 < count_best_model1 & count_best_model3 < count_best_model2){
    count_best_model3 <- count_best_model3 + 1
  }

  MSE_deg1[i] <- mse_test_deg1
  MSE_deg2[i] <- mse_test_deg2
  MSE_deg3[i] <- mse_test_deg3
  
}
```

```{r}

```

```{}
```

Add solution here
:::

## h. Function to implement simulation

Write a function that implements the simulation in part *f*. The function should have arguments for i) the size of the training data $n$, ii) the standard deviation of the random error $\sigma$, and iii) the test data. Use the same `set.seed(613)`.

::: {.callout-note title="Solution"}
Add solution here
:::

## i. Performance when $\sigma=2$

Use your function to repeat the simulation in part *f*, but use $\sigma=2$. Report the number of times each model was best (you do not need to produce any plots).

-   Be sure to generate new test data with ($n = 10000$, $\sigma = 2$, using `seed = 612`).

::: {.callout-note title="Solution"}
Add solution here
:::

## j. Performance when $\sigma=4$ and $n=300$

Repeat *i*, but now use $\sigma=4$ and $n=300$.

-   Be sure to generate new test data with ($n = 10000$, $\sigma = 4$, using `seed = 612`).

::: {.callout-note title="Solution"}
Add solution here
:::

## k. Understanding

Describe the effects $\sigma$ and $n$ has on selection of the best model? Why is the *true* model form (i.e., quadratic) not always the *best* model to use when prediction is the goal?

::: {.callout-note title="Solution"}
Add solution here
:::
