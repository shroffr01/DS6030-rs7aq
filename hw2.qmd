---
title: "Homework #2: Resampling" 
author: "**Rohan Shroff**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/teaching/data/' # data directory

library(tidymodels)# for optional tidymodels solutions
library(tidyverse) # functions for data manipulation  
```

# Problem 1: Bootstrapping

Bootstrap resampling can be used to quantify the uncertainty in a fitted curve.

## a. Data Generating Process

Create a set of functions to generate data from the following distributions: \begin{align*}
X &\sim \mathcal{U}(0, 2) \qquad \text{Uniform between $0$ and $2$}\\
Y &= 1 + 2x + 5\sin(5x) + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma=2.5)
\end{align*}

::: {.callout-note title="Solution"}
```{r}
# generates random x values
sim_x <- function(n) {
  runif(n, min=0, max=2)
}

# defines function y
f <- function(x) {
  1 + 2*x + 5*sin(5*x)
}

# generates y values based on x
sim_y <- function(x) {
  n = length(x)
  f(x) + rnorm(n, sd = 2.5)
}
```

Add solution here
:::

## b. Simulate data

Simulate $n=100$ realizations from these distributions. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$. Use `set.seed(211)` prior to generating the data.

::: {.callout-note title="Solution"}
```{r}
set.seed(211)

n <- 100

x <- sim_x(100)
y <- sim_y(x)

data_train <- tibble(x,y)
```

```{r}
ggplot(data_train, aes(x,y)) + geom_point() + 
  geom_function(fun = f, color = 'red', linewidth=1) +
  scale_x_continuous(breaks=seq(-2,2, by=1)) +
  scale_y_continuous(breaks=seq(-10,10,by=2)) #+ 
  #ggtitle("Plot")+
  #xlab("x") + ylab("y")
```
:::

## c. 5th degree polynomial fit

Fit a 5th degree polynomial. Produce a scatterplot and draw the *estimated* regression curve.

::: {.callout-note title="Solution"}
Add solution h

```{r}
deg_5_model <- lm(y~poly(x, degree=5), data=data_train)
```

```{r}

ggplot(data_train, aes(x,y)) + geom_point(size=2, alpha=0.5) + 
  geom_function(fun = f, color = 'black', size = 1) +
  stat_smooth(method = "lm", formula = y ~ poly(x, 5), aes(color = "5th Degree fit"), se = FALSE)  +
  scale_x_continuous(breaks=seq(-3,3, by=.5)) +
  scale_y_continuous(breaks=seq(-50,50,by=2))  

```
:::

## d. Bootstrap sampling

Make 200 bootstrap samples. For each bootstrap sample, fit a 5th degree polynomial and make predictions at `eval_pts = seq(0, 2, length=100)`

-   Set the seed (use `set.seed(212)`) so your results are reproducible.
-   Produce a scatterplot with the original data and add the 200 bootstrap curves

::: {.callout-note title="Solution"}
```{r}
set.seed(212)

M <- 200
data_eval <-  tibble(x=seq(0, 2, length=100))
YHAT <-  matrix(NA, nrow(data_eval), M)

for(m in 1:M){
# sample indices/rows from empirical distribution (with replacement)
ind = sample(n, replace=TRUE)
# fit bspline model to those indices/rows
m_boot <-  lm(y~poly(x, degree=5), data=data_train[ind,])
#- predict from bootstrap model
YHAT[,m] = predict(m_boot, data_eval)
}
```

```{r}
data_fitted <- as_tibble(YHAT) %>% # convert matrix to tibble
bind_cols(data_eval) %>% # add the eval points
pivot_longer(-x, names_to="simulation", values_to="y") # convert to long format


ggplot(data_train, aes(x,y)) +
geom_line(data=data_fitted, color="red", alpha=.10, aes(group=simulation)) +
geom_point()
```
:::

## e. Confidence Intervals

Calculate the pointwise 95% confidence intervals from the bootstrap samples. That is, for each $x \in {\rm eval\_pts}$, calculate the upper and lower limits such that only 5% of the curves fall outside the interval at $x$.

-   Remake the plot from part *c*, but add the upper and lower boundaries from the 95% confidence intervals.

::: {.callout-note title="Solution"}
```{r}
data_org <- data_eval %>%
  mutate(
    lower=apply(YHAT,1,quantile,probs=0.05),
    upper=apply(YHAT,1,quantile,probs=0.95)
  )
```

```{r}
data_fitted <- as_tibble(YHAT) %>% # convert matrix to tibble
bind_cols(data_eval) %>% # add the eval points
pivot_longer(-x, names_to="simulation", values_to="y") # convert to long format


ggplot(data_train, aes(x,y)) +
geom_line(data=data_fitted, color="black", alpha=.10, aes(group=simulation)) +
  geom_ribbon(aes(ymin=lower, ymax=upper), data=data_org,fill='red',alpha=0.4)+
geom_point()
```
:::

# Problem 2: V-Fold cross-validation with $k$ nearest neighbors

Run 10-fold cross-validation on the data generated in part 1b to select the optimal $k$ in a k-nearest neighbor (kNN) model. Then evaluate how well cross-validation performed by evaluating the performance on a large test set. The steps below will guide you.

## a. Implement 10-fold cross-validation

Use $10$-fold cross-validation to find the value of $k$ (i.e., neighborhood size) that provides the smallest cross-validated MSE using a kNN model.

-   Search over $k=3,4,\ldots, 40$.
-   Use `set.seed(221)` prior to generating the folds to ensure the results are replicable.
-   Show the following:
    -   the optimal $k$ (as determined by cross-validation)
    -   the corresponding estimated MSE
    -   produce a plot with $k$ on the x-axis and the estimated MSE on the y-axis (optional: add 1-standard error bars).
-   Notation: The $k$ is the tuning paramter for the kNN model. The $v=10$ is the number of folds in V-fold cross-validation. Don't get yourself confused.

::: {.callout-note title="Solution"}
```{r}
library(FNN)

knn_eval <- function(k, data_train, data_test){
# fit model and eval on training data
knn = knn.reg(data_train[,'x', drop=FALSE],
y = data_train$y,
test = data_train[,'x', drop=FALSE],
k = k)
r = data_train$y-knn$pred # residuals on training data
mse_train = mean(r^2) # training MSE
# fit model and eval on test data
knn.test = knn.reg(data_train[,'x', drop=FALSE],
y = data_train$y,
test = data_test[,'x', drop=FALSE],
k = k)
r_test = data_test$y - knn.test$pred # residuals on test data
mse_test = mean(r_test^2) # test MSE
# results
edf = nrow(data_train)/k # effective dof (edof)
output <- tibble(k = k, edf = edf, mse_train, mse_test)
}
```

```{r}
data_train
```

```{r}
set.seed(221)

folds <- 10
fold <-  sample(rep(1:folds, length=nrow(data_train))) 

k_MSE_vals <- vector()
edf_vals_m <- vector()

for (k in 1:38){

  k <- k+2
  MSE_vals <- vector()
  edf_vals <- vector()

  for(j in 1:folds){
    
    #-- Set training/val data
    val = which(fold == j) # indices of holdout/validation data
    train = which(fold != j) # indices of fitting/training data
    n.val = length(val) # number of observations in validation
    
    #- fit and evaluate models
    data_fit = slice(data_train, train)
    data_eval = slice(data_train, val)
    
    output <- knn_eval(k, data_fit, data_eval)
    MSE_vals[j] <- output['mse_test']
    edf_vals[i] <- output['edf']
  
  }

  MSE_vals <- unlist(MSE_vals)
  mean_MSE <- mean(MSE_vals)
  
  edf_vals <- unlist(edf_vals)
  mean_edf <- mean(edf_vals)
  
  k <- k-2
  
  k_MSE_vals[k] <- mean_MSE
  edf_vals_m[k] <- mean_edf
}
```

```{r}
k_seq <- seq(3,40,1)

data_df <- tibble(k_seq, k_MSE_vals)

ggplot(data=data_df, aes(x=k_seq,y=k_MSE_vals)) + 
  geom_line(color='blue', linewidth=2)

```
:::

## b. Find the optimal *edf*

The $k$ (number of neighbors) in a kNN model determines the effective degrees of freedom *edf*. What is the optimal *edf*? Be sure to use the correct sample size when making this calculation. Produce a plot similar to that from part *a*, but use *edf* (effective degrees of freedom) on the x-axis.

::: {.callout-note title="Solution"}
```{r}
k_seq <- seq(3,40,1)

data_df_edf <- tibble(edf_vals_m, k_MSE_vals)

ggplot(data=data_df, aes(x=edf_vals_m,y=k_MSE_vals)) + 
  geom_line(color='blue', linewidth=2)
```

Add solution here
:::

## c. Choose $k$

After running cross-validation, a final model fit from *all* of the training data needs to be produced to make predictions. What value of $k$ would you choose? Why?

::: {.callout-note title="Solution"}
I would choose 9 as my k for the KNN model, because this is where the lowest test MSE begins and also when the model is simpler compared to k = 10, or 11.
:::

## d. Evaluate actual performance

Now we will see how well cross-validation performed. Simulate a test data set of $50000$ observations from the same distributions. Use `set.seed(223)` prior to generating the test data.

-   Fit a set of kNN models, using the full training data, and calculate the mean squared error (MSE) on the test data for each model. Use the same $k$ values in *a*.
-   Report the optimal $k$, the corresponding *edf*, and MSE based on the test set.

::: {.callout-note title="Solution"}
```{r}
n_test <- 50000
set.seed(223)

x_test <- sim_x(n_test)
y_test <- sim_y(x_test)
data_test <- tibble(x=x_test, y=y_test)

MSE_knn <- vector()

for (k in 1:38){
  k <- k+2
  output <- knn_eval(k, data_train, data_test)
    
  k <- k-2
  MSE_knn[k] <- output['mse_test']
}
```

```{r}
MSE_knn <- unlist(MSE_knn)
best_k <- which.min(MSE_knn) + 2
edf <- nrow(data_train)/best_k
print(paste('Best performing k is', best_k))
print(paste('MSE value for k= 13', MSE_knn[11]))
print(paste('EDF value for k= 13', edf))
```

Add solution here
:::

## e. Performance plots

Plot both the cross-validation estimated and (true) error calculated from the test data on the same plot. See Figure 5.6 in ISL (pg 182) as a guide.

-   Produce two plots: one with $k$ on the x-axis and one with *edf* on the x-axis.
-   Each plot should have two lines: one from part *a* and one from part *d*

::: {.callout-note title="Solution"}
Add solution here
:::

## f. Did cross-validation work as intended?

Based on the plots from *e*, does it appear that cross-validation worked as intended? How sensitive is the choice of $k$ on the resulting test MSE?

::: {.callout-note title="Solution"}
```{r}

```
:::
