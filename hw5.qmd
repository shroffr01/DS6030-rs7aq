---
title: "Homework #5: Probability and Classification" 
author: "**Rohan Shroff**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
dir_data= 'https://mdporter.github.io/teaching/data/' # data directory
library(glmnet)
library(tidyverse) # functions for data manipulation 
library(ranger)       # fast random forest implementation
library(pROC)
```

# Crime Linkage

Crime linkage attempts to determine if a set of unsolved crimes share a common offender. *Pairwise* crime linkage is the more simple task of deciding if two crimes share a common offender; it can be considered a binary classification problem. The linkage training data has 8 evidence variables that measure the similarity between a pair of crimes:

-   `spatial` is the spatial distance between the crimes
-   `temporal` is the fractional time (in days) between the crimes
-   `tod` and `dow` are the differences in time of day and day of week between the crimes
-   `LOC`, `POA,` and `MOA` are binary with a 1 corresponding to a match (type of property, point of entry, method of entry)
-   `TIMERANGE` is the time between the earliest and latest possible times the crime could have occurred (because the victim was away from the house during the crime).
-   The response variable indicates if the crimes are linked ($y=1$) or unlinked ($y=0$).

These problems use the [linkage-train](%60r%20file.path(dir_data,%20%22linkage_train.csv%22)%20%60) and [linkage-test](%60r%20file.path(dir_data,%20%22linkage_test.csv%22)%20%60) datasets (click on links for data).

## Load Crime Linkage Data

::: {.callout-note title="Solution"}
```{r}
linkage_train <- read.csv('linkage_train.csv')
linkage_test <- read.csv('linkage_test.csv')
```

```{r}
linkage_train
```
:::

# Problem 1: Penalized Regression for Crime Linkage

## a. Fit a penalized *linear regression* model to predict linkage.

Use an elastic net penalty (including lasso and ridge) (your choice).

-   Report the value of $\alpha \in [0, 1]$ used.
-   Report the value of $\lambda$ used.
-   Report the estimated coefficients.

::: {.callout-note title="Solution"}
```{r}
# Run analysis to determine what alpha and lambda value results in best performance (first generate training model, and then use test dataset to evaluate)

# finding minimum alpha and lambda based on training data
alpha <- seq(0,1,0.05)

k <- 10
folds = rep(1:k, length=nrow(linkage_train))

min_MSE_list <- vector()
min_lambda_list <- vector()

for (a in 1:length(alpha)){
  
x_train <- as.matrix(linkage_train[,-9])
y_train <- linkage_train$y

rs_train_model <- cv.glmnet(x_train, y_train, alpha=alpha[a], foldid = folds)

lambda_min <- rs_train_model$lambda.min


min_lambda_index <- which(rs_train_model$lambda == lambda_min)
min_mse <- rs_train_model$cvm[min_lambda_index]

min_MSE_list[a] <- min_mse
min_lambda_list[a] <- lambda_min

}

min_MSE_list <- unlist(min_MSE_list)
a <- which.min(min_MSE_list)
a

min_lambda_list <- unlist(min_lambda_list)
b <- min_lambda_list[a]
b


```

The best performance corresponds to an alpha value of 0.05 and a lambda of 0.00069.

```{r}
# Fitting lm model
linkage_opt_model <- glmnet(x_train, y_train, alpha=0.05, lambda=b)
linkage_opt_model$beta
```

```{r}
linkage_opt_model$nobs
```
:::

## b. Fit a penalized *logistic regression* model to predict linkage.

Use an elastic net penalty (including lasso and ridge) (your choice).

-   Report the value of $\alpha \in [0, 1]$ used.
-   Report the value of $\lambda$ used.
-   Report the estimated coefficients.

::: {.callout-note title="Solution"}
```{r}
linkage_opt_model_log <- glmnet(x_train, y_train, family='binomial', alpha=0, lambda=b)
linkage_opt_model_log$beta
```
:::

# Problem 2: Random Forest for Crime Linkage

Fit a random forest model to predict crime linkage.

-   Report the loss function (or splitting rule) used.
-   Report any non-default tuning parameters.
-   Report the variable importance (indicate which importance method was used).

::: {.callout-note title="Solution"}
```{r}
linkage_opt_model_rf <- ranger(y~., num.trees=1000, mtry = 3, importance = 'impurity', data=linkage_train)
```

The loss function I used is the Gini method (default). The variable importance was determined by impurity (default). I used mtry of 3.
:::

# Problem 3: ROC Curves

## a. ROC curve: training data

Produce one plot that has the ROC curves, using the *training data*, for all three models (linear, logistic, and random forest). Use color and/or linetype to distinguish between models and include a legend.\
Also report the AUC (area under the ROC curve) for each model. Again, use the *training data*.

-   Note: you should be weary of being asked to evaluation predictive performance from the same data used to estimate the tuning and model parameters. The next problem will walk you through a more proper way of evaluating predictive performance with resampling.

::: {.callout-note title="Solution"}
```{r}
# Generate training data predictions
linkage_train_mat <- as.matrix(linkage_train[,-9])

predictions_lin <- predict(linkage_opt_model, linkage_train_mat, type = 'response')
predictions_log <- predict(linkage_opt_model_log, linkage_train_mat, type = 'response')
predictions_rf <- predict(linkage_opt_model_rf, linkage_train_mat, type = 'response')
```

```{r}
predictions_lin <- as.numeric(predictions_lin)
predictions_log <- as.numeric(predictions_log)
predictions_rf <- as.numeric(predictions_rf$predictions)
```

```{r}
roc_lin <- roc(linkage_train[,9], predictions_lin)
roc_log <- roc(linkage_train[,9], predictions_log)
roc_rf <- roc(linkage_train[,9], predictions_rf)
```

```{r}
roc_lin_data <- data.frame(FPR = (1-roc_lin$specificities), TPR = roc_lin$sensitivities, model = 'Linear model')
roc_log_data <- data.frame(FPR = (1-roc_log$specificities), TPR = roc_log$sensitivities, model = 'Logistic model')
roc_rf_data <- data.frame(FPR = (1-roc_rf$specificities), TPR = roc_rf$sensitivities, model = 'Random Forest model')

roc_df <- rbind(roc_lin_data, roc_log_data, roc_rf_data)
```

```{r}
ggplot(roc_df, aes(x=FPR)) + 
  geom_line(aes(y=TPR, color = model)) + 
  labs(title = 'Plot', x = 'FPR', y = 'TPR')
```

```{r}
print(roc_lin$auc) # linear
print(roc_log$auc) # logistic
print(roc_rf$auc) # random forest
```
:::

## b. ROC curve: resampling estimate

Recreate the ROC curve from the penalized logistic regression (logreg) and random forest (rf) models using repeated hold-out data. The following steps will guide you:

-   For logreg, use $\alpha=.75$. For rf use *mtry = 2*, *num.trees = 1000*, and fix any other tuning parameters at your choice.
-   Run the following steps 25 times:
    i.  Hold out 500 observations.
    ii. Use the remaining observations to estimate $\lambda$ using 10-fold CV for the logreg model. Don't tune any rf parameters.
    iii. Predict the probability of linkage for the 500 hold-out observations.
    iv. Store the predictions and hold-out labels.
    v.  Calculate the AUC.
-   Report the mean AUC and standard error for both models. Compare to the results from part a.
-   Produce two plots showing the 25 ROC curves for each model.
-   Note: by estimating $\lambda$ each iteration, we are incorporating the uncertainty present in estimating that tuning parameter.

::: {.callout-note title="Solution"}
```{r}

K <- 10
folds = rep(1:K, length=nrow(linkage_train)-500)

# initializing empty dataframe to append roc information
results_df_log <- data.frame()
results_df_rf <- data.frame()

hold_out_labels <- list()

predictions_log_list <- list()
predictions_rf_list <- list()

auc_log_list <- list()
auc_rf_list <- list()

for (i in (1:25)){
   hold_out <- sample(1:nrow(linkage_train), 500, replace=FALSE)
   
   linkage_train_modified <- linkage_train[-hold_out, ]
     
    x_train <- as.matrix(linkage_train_modified[,-9])
    y_train <- linkage_train_modified[,9]
    
    # logistic regression model 
    
    linkage_holdout_model_log <- cv.glmnet(x_train, y_train, family='binomial', alpha=0.75, foldid=folds)
    
    lambda_min <- linkage_holdout_model_log$lambda.min
    
    x_test <- as.matrix(linkage_train[hold_out,-9])
    
    yhat_lambda_min_log = predict(linkage_holdout_model_log, x_test, type = 'response', s = "lambda.min")
    predictions_log <- as.numeric(yhat_lambda_min_log)
    
    # Generating roc 
    roc_log <- roc(linkage_train[hold_out, 9], predictions_log)
    roc_log_auc <- roc_log$auc
    
    roc_log_data <- data.frame(FPR = (1-roc_log$specificities), TPR = roc_log$sensitivities, model = 'Logistic model', label=i)
    
    # appending roc data to main data frame
    results_df_log <- rbind(results_df_log, roc_log_data)

    # storing hold out values for reference
    hold_out_labels[[i]] <- hold_out
    
    # appending predictions and auc values
    predictions_log_list[[i]] <- predictions_log
    auc_log_list[i] <- roc_log_auc
    
    # random forest model 
    
    linkage_holdout_model_rf <- ranger(x=linkage_train_modified[,-9],y=linkage_train_modified[,9], num.trees=1000, mtry = 2)
    
    x_test_rf <- as.matrix(linkage_train[hold_out,-9])
    
    predictions_rf <- predict(linkage_holdout_model_rf, x_test_rf, type = 'response')
    predictions_rf <- as.numeric(predictions_rf$predictions)
    
    # Generating roc 
    roc_rf <- roc(linkage_train[hold_out, 9], predictions_rf)
    roc_rf_auc <- roc_rf$auc
    
    roc_rf_data <- data.frame(FPR = (1-roc_rf$specificities), TPR = roc_rf$sensitivities, model = 'Random Forest model', label=i)
    
    # appending roc data to main data frame
    results_df_rf <- rbind(results_df_rf, roc_rf_data)
    
    # appending predictions and auc values
    predictions_rf_list[[i]] <- predictions_rf
    auc_rf_list[i] <- roc_rf_auc

}

```

```{r}
# Mean AUC
auc_log_list <- unlist(auc_log_list)
auc_rf_list <- unlist(auc_rf_list)

print(mean(auc_log_list))
print(mean(auc_rf_list))
```

```{r}
# Standard Error AUC

print(sd(auc_log_list)/sqrt(length(auc_log_list)))
print(sd(auc_rf_list)/sqrt(length(auc_rf_list)))
```

```{r}
# ROC curve for Logistic model

results_df_log$label <- as.factor(results_df_log$label)

ggplot(results_df_log, aes(x=FPR)) + 
  geom_line(aes(y=TPR, color = label)) + 
  geom_abline(slope=1, intercept=0, linetype='dashed', color='black') +
  labs(title = 'ROC Curve Logistic Regression', x = 'FPR', y = 'TPR')
```

```{r}
# ROC curve for Random Forest model

results_df_rf$label <- as.factor(results_df_rf$label)

ggplot(results_df_rf, aes(x=FPR)) + 
  geom_line(aes(y=TPR, color = label)) + 
  geom_abline(slope=1, intercept=0, linetype='dashed', color='black') +
  labs(title = 'ROC Curve Random Forest', x = 'FPR', y = 'TPR')
```
:::

# Problem 4: Contest

## a. Contest Part 1: Predict the estimated *probability* of linkage.

Predict the estimated *probability* of linkage for the test data (using any model).

-   Submit a .csv file (ensure comma separated format) named `lastname_firstname_1.csv` that includes the column named **p** that is your estimated posterior probability. We will use automated evaluation, so the format must be exact.
-   You are free to any model (even ones we haven't yet covered in the course).
-   You are free to use any data transformation or feature engineering.
-   You will receive credit for a proper submission; the top five scores will receive 2 bonus points.\
-   Your probabilities will be evaluated with respect to the mean negative Bernoulli log-likelihood (known as the average *log-loss* metric): $$ 
    L = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
    $$ where $M$ is the number of test observations, $\hat{p}_i$ is the prediction for the $i$th test observation, and $y_i \in \{0,1\}$ are the true test set labels.

::: {.callout-note title="Solution"}
```{r}
p <- predict(linkage_opt_model_rf, linkage_test, type = 'response')

write.csv(p$predictions, "C:/Users/shrof/R/UVA/DS6030-rs7aq/Shroff_Rohan1.csv", row.names=TRUE)
```
:::

## b. Contest Part 2: Predict the *linkage label*.

Predict the linkages for the test data (using any model).

-   Submit a .csv file (ensure comma separated format) named `lastname_firstname_2.csv` that includes the column named **linkage** that takes the value of 1 for linked pairs and 0 for unlinked pairs. We will use automated evaluation, so the format must be exact.
-   You are free to any model (even ones we haven't yet covered in the course).
-   You are free to use any data transformation or feature engineering.
-   Your labels will be evaluated based on total cost, where cost is equal to `1*FP + 8*FN`. This implies that False Negatives (FN) are 8 times as costly as False Positives (FP).\
-   You will receive credit for a proper submission; the top five scores will receive 2 bonus points. Note: you only will get bonus credit for one of the two contests.

::: {.callout-note title="Solution"}
```{r}
p1 <- predict(linkage_opt_model_rf, linkage_test, type='response')
p1_class <- ifelse(p1$predictions > 0.15, 1, 0)

write.csv(p1_class, "C:/Users/shrof/R/UVA/DS6030-rs7aq/Shroff_Rohan2.csv", row.names=FALSE)
```
:::
