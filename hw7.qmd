---
title: "Homework #7: Stacking and Boosting" 
author: "**Rohan Shroff**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
library(ranger)
library(dplyr)
library(tidyverse)
library(caret)
library(glmnet)
library(xgboost)
```

# Stacking for Kaggle

You are to make at least one official entry in the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) Kaggle contest **using stacking or model averaging**; at least one component model must be a boosting model.

-   You will need to register in Kaggle (its free)
-   Read the details of the contest. Understand the data and evaluation function.
-   Make at least one submission that uses **stacking or model averaging**.
-   If you get a score on the public leaderboard of $\text{RMSE}<0.50$ (note RMSE is calculated on the log scale), you receive full credit, otherwise, you'll lose 10 points.
    -   I'll allow [teaming](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/team). Each team member can produce one component model and then use stacking or model averaging to combine predictions.
    -   You don't need to team, but must still combine multiple models. At least one of the component models should be boosting.
-   Each person submit the following in Canvas:
    -   Code (if teaming, your code and the shared stacking code)
    -   kaggle name (or team name) so we can ensure you had a valid submission.
    -   your score and current ranking on the kaggle leaderboard
-   Top 5 scores get 2 bonus points
    -   Teams will split their bonus points among team members

Note: Check out the [Kaggle notebooks](https://www.kaggle.com/docs/notebooks) which let you make submissions directly from the notebook. Its very similar to using Rivanna's OnDemand in that you can make a RMarkdown/Jupyter notebook or R/Python scrips that run on the cloud. Free CPU (4 cores, 30GB RAM) - amazing! Let your laptops cool off after all their hard work this semester.

**Kaggle Name: FirstinWeather, score: 0.13622, 1486/4940**

**Data Import and Cleaning**

```{r}
df_train <- read.csv('train.csv')
df_test <- read.csv('test.csv')

# columns to omit 
df_train <- df_train %>% select(-MiscFeature, -PoolQC)
df_test <- df_test %>% select(-MiscFeature, -PoolQC)
```

```{r}
df_train <-  df_train %>%
  mutate(across(where(is.character), ~ replace_na(., "missing")))

df_test <-  df_test %>%
  mutate(across(where(is.character), ~ replace_na(., "missing")))
```

```{r}
df_train$SalePrice <- log(df_train$SalePrice)
```

```{r}
# replace missing int values with mean of column

df_train <- df_train %>%
  mutate(across(where(is.numeric), ~ replace_na(., round(mean(.,na.rm=TRUE)))))

df_test <- df_test %>%
  mutate(across(where(is.numeric), ~ replace_na(., round(mean(.,na.rm=TRUE)))))
```

```{r}
# hot encode all categorical columns

dmy <- dummyVars(" ~ .", data = df_train)
df_train_mod <- data.frame(predict(dmy, newdata = df_train))

df_train_int <- df_train %>% select(where(is.numeric))

dmy <- dummyVars(" ~ .", data = df_test)
df_test_mod <- data.frame(predict(dmy, newdata = df_test))

df_test_int <- df_test %>% select(where(is.numeric))
```

```{r}
same_columns <- intersect(names(df_train_mod), names(df_test_mod))
cols_to_keep <- unique(c(same_columns)) 

df_train_mod_f <- df_train_mod[, same_columns]
df_train_mod_f['SalePrice'] = df_train_mod[, 'SalePrice']

df_test_mod_f <- df_test_mod[, cols_to_keep]   
```

```{r}
nrow(df_test_mod_f)
```

**Fitting Models**

```{r}
# fitting random forest model to training data

rf_model <- ranger(SalePrice~., data = df_train_mod_f)
rf_model$prediction.error

predictions_rf <- rf_model$predictions
```

```{r}
# linear regression model 

lin_model <- lm(SalePrice~., data=df_train_mod_f)
mean((lin_model$residuals)**2)

predictions_lin <- predict(lin_model, df_train_mod_f)
```

```{r}
# lasso regression model

x_train <- as.matrix(df_train_mod_f[,-ncol(df_train_mod_f)])
y_train <- df_train_mod_f$SalePrice

lasso_model <- glmnet(x_train, y_train, alpha=0.7, lambda=0)
predictions_lasso <- predict(lasso_model, x_train)
mse <- mean((df_train_mod_f$SalePrice - predictions_lasso)^2)
mse
```

```{r}
# XG Boost model

params <- list(objective = "reg:squarederror", eval_metric = 'rmse', max_depth=1, eta = 0.2)

xg_boost_model <- xgboost(data=x_train, label=y_train, params = params, nrounds=5000, verbose=0)
rmse = xg_boost_model$evaluation_log$train_rmse[5000]
rmse

predictions_xg <- predict(xg_boost_model, newdata = x_train)
```

**Linear Stacking**

```{r}
# create data frame with Sale Price and predictions from each model for training data

stack_df <- data.frame(predictions_rf, predictions_lin, predictions_lasso, predictions_xg, df_train_mod_f['SalePrice'])
```

```{r}
# Use linear stacking on training data set by fitting linear model
stack_lin_model <- lm(SalePrice~., data = stack_df)
summary(stack_lin_model)
```

```{r}
# Generate predictions based on test data for each model 

rf_pred_test <-  predict(rf_model, data = df_test_mod_f) # random forest
lin_pred_test <-  predict(lin_model, df_test_mod_f) # lin reg

x_test <- as.matrix(df_test_mod_f)
lasso_pred_test <-  predict(lasso_model, newx = x_test) # lasso 
xg_boost_pred_test <- predict(xg_boost_model, newdata=x_test)
```

```{r}
# get finalized predictions by applying weights obtained in training data set
pred_final <- -0.060842*(rf_pred_test$predictions) + 0.0703608*(lin_pred_test) + -0.157767*(lasso_pred_test) + 1.139946*(xg_boost_pred_test)

pred_final <- exp(pred_final)
```

```{r}
pred_final1 <- 0.05*(rf_pred_test$predictions) + 0.05*(lin_pred_test) + 0.05*(lasso_pred_test) + 0.85*(xg_boost_pred_test)

pred_final1 <- exp(pred_final1)
```

```{r}
#write.csv(pred_final, "C:/Users/shrof/R/UVA/DS6030-rs7aq/submission.csv")
```

```{r}
#write.csv(pred_final1, "C:/Users/shrof/R/UVA/DS6030-rs7aq/submission3.csv")
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```
